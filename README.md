# Supplementary Materials for the Article "Human Experts vs. Large Language Models: Evaluating Annotation Scheme and Guidelines Development for Clinical Narratives"

This repository contains supplementary materials for the scientific article **"Human Experts vs. Large Language Models: Evaluating Annotation Scheme and Guidelines Development for Clinical Narratives "**. These documents provide detailed insights and data supporting the research and are structured as follows:

- **Inputs and Outputs of Prompts**: The inputs and their corresponding outputs generated during the experiments.
- **Synthetic Reports for Annotation**: A set of synthetic reports that were used as reference materials for the annotation process.
- **Synthetic Report as Appendix in Prompts 3**: A specific synthetic report that is included as an appendix in the third prompt of the study.
- **Text2Story Annotation Scheme**: The annotation scheme for temporal annotation (Text2Story), used as an appendix to prompts 2 and 3, outlining the methodology and process for annotation.
- **Human Expert Annotation Scheme**: The annotation scheme developed by the human expert, serving as a baseline for the comparison and evaluation of annotations.
- **Evaluations of Guidelines**: Evaluations of the guidelines developed by both the human expert and the LLM, using Likert scales filled out by annotators to assess their clarity and effectiveness.
- **Evaluation of LLM Guidelines**: An assessment of the guidelines generated by the LLM, using Likert scales completed by the human expert.

These documents are intended to support the findings presented in the article and provide transparency regarding the methodologies and evaluations used in the study.


Thank you for your interest in our work!
